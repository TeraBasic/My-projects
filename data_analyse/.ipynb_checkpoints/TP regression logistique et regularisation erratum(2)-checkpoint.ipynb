{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TP - Régression et Régularisation\n",
    "Dans ce TP vous devrez implémenter la regression logistique (en partant du code de la régression linéaire) et vous l'appliquerez sur des données décrivant des voitures (`Auto2.csv`)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### D'abord, chargeons les données\n",
    "\n",
    "Les données décrivent des voitures.\n",
    "On a des variables comme le poids de la voiture, son accelération, etc...\n",
    "et on cherche à prédire sa cylindrée, qui peut être 4 cylindres (classe 0) ou plus (classe 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "from scipy.stats import mode\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.utils import shuffle\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour charger les données depuis un fichier csv, on utilise un module python qui s'appelle `pandas`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas\n",
    "autos = pandas.read_csv( \"Auto2.csv\")\n",
    "autos.drop(labels=['name','origin'],axis=1,inplace=True)\n",
    "autos = autos[autos.cylinders != 3]\n",
    "autos = autos[autos.cylinders != 5]\n",
    "autos.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "maintenant, on converti ces données en tableaux numpy:\n",
    "* `X` sera le tableau de données à 5 variables\n",
    "* la cylindrée sera stockée dans `y`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = autos[['mpg','displacement','weight','acceleration','year']].as_matrix()\n",
    "y = (autos[['cylinders']].as_matrix().squeeze() >= 6).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print('X=',X,'\\ny=',y[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Affichons les points de X, en utilisant seulement les 3ieme et 4ieme colonnes de X.\n",
    "plt.scatter(X[:,2],X[:,3],c=y)\n",
    "plt.xlabel('poids')\n",
    "plt.ylabel('acceleration');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sujet du TP\n",
    "\n",
    "\n",
    "Avant tout, on va mélanger les données avec la commande `X,y = shuffle(X,y)`. Cette commande provient du module python `scikit-learn`\n",
    "\n",
    "#### Régression\n",
    "\n",
    "* Lancez la fonction `descente_grad_stochatstique_reg_lineaire` (ci-dessous en annexe) avec les bons paramètres (que vous trouverez par essai-erreur).  Vous lirez le code pour comprendre parfaitement ce qu'elle fait. Remarquez que les données ne sont pas normalisées, et ont des valeurs relativement grandes. Donc il est probable que seul un très petit pas d'apprentissage convienne ici.\n",
    "* Notre problème est un problème de classification, donc la régression linéaire n'est pas adaptée. Ce qu'on cherche à faire est plutot une régression logistique. Ecrivez la fonction `descente_grad_stochatstique_reg_logistique` en modifiant la fonction `descente_grad_stochatstique_reg_lineaire`, et lancez-la. Testez la regression logistique associée sur les données.\n",
    "* Ecrivez une fonction  `erreur_empirique(X,y,theta)` qui calcule l'erreur quadratique empirique de la regression logistique sur l'ensemble des données `X,y`\n",
    "* Ecrivez une fonction  `log_vraissemblance_empirique(X,y,theta)` qui calcule le log de la vraissemblance empirique de la regression logistique sur l'ensemble des données `X,y`\n",
    "\n",
    "#### Régularisation\n",
    "\n",
    "* Ajoutez une régularisation l1 à la descente de gradient. Pour rappel, la norme l1 est la somme des valeurs absolues des theta_j. Donc dans la formule du gradient, il faut ajouter lambda*signe(theta_j) pour chaque coordonnée j (ce qui correspond à la dérivée de la norme l1).\n",
    "* Pour différentes valeurs du paramètre lambda, relancez la régression logistique. Affichez en fonction de lambda le taux d'erreur en classification, et le nombre de theta_j non-nuls (en réalité, les theta_j ne sont jamais exactement égaux à zero. Donc on choisira un petit epsilon, et on comptera le nombre de theta_j donc la valeur absolue est plus grande que epsilon).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Annexes\n",
    "\n",
    "Fonctions qui seront utiles pour faire le TP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Pour faire la descente de gradient, on a besoin de la fonction g(z)=1/(1+exp(-z))\n",
    "# mais cette fonction est \"numeriquement instable\", car l'exponentiel peut générer des\n",
    "# valeurs hors des limites des floats.\n",
    "# Donc on utilise la version suivante de g(), équivalente mais stable\n",
    "\n",
    "def g(z):\n",
    "    \"Numerically stable sigmoid function.\"\n",
    "    if z >= 0:\n",
    "        ez = np.exp(-z)\n",
    "        return 1 / (1 + ez)\n",
    "    else:\n",
    "        # if x is less than zero then z will be small, denom can't be\n",
    "        # zero because it's 1+z.\n",
    "        ez = np.exp(z)\n",
    "        return ez / (1 + ez)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Voici un algorithme de base de descente de gradient pour une régression lineaire appliqué aux données.\n",
    "# NB: pour simplifier, le modèle prédit n'a pas de constante theta0 comme vu en cours.\n",
    "\n",
    "def descente_grad_stochatstique_reg_lineaire(X,y,alpha):\n",
    "\n",
    "    n,d = X.shape\n",
    "    theta = np.zeros(d)\n",
    "\n",
    "    for t in range(1000):\n",
    "\n",
    "        i = random.randint(0,n-1)\n",
    "        xi = X[i]\n",
    "        yi = y[i]\n",
    "        h = np.dot( theta , xi )\n",
    "\n",
    "        theta -= alpha*xi*(h-yi)\n",
    "\n",
    "    return theta"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
